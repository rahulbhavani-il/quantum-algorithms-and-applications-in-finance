# HHL Algorithm - Solving Linear Systems Quantum Fast ‚ö°

> **Exponential Speedup for Linear Algebra (With Caveats)**
> **Difficulty**: üî¥ Advanced | **Time**: 4-5 hours | **Prerequisites**: QPE
> **Codelab**: [11_hhl_codelab.ipynb](codelabs/11_hhl_codelab.ipynb)

---

## üéØ Learning Objectives

By the end of this section, you will be able to:

1. ‚úÖ Understand the HHL algorithm's exponential speedup claim
2. ‚úÖ Recognize the critical caveats (qRAM, output, condition number)
3. ‚úÖ Trace through the HHL circuit: QPE ‚Üí controlled rotation ‚Üí inverse QPE
4. ‚úÖ Implement a simplified HHL for 2√ó2 systems
5. ‚úÖ Identify when HHL provides genuine quantum advantage

---

## üìã Table of Contents

1. [WHY: The Linear Systems Revolution](#why-the-linear-systems-revolution)
2. [WHAT: HHL Core Concepts](#what-hhl-core-concepts)
3. [The HHL Algorithm Step-by-Step](#the-hhl-algorithm-step-by-step)
4. [Critical Caveats](#critical-caveats)
5. [Implementation Details](#implementation-details)
6. [Trap Alerts](#trap-alerts)
7. [Quick Checks](#quick-checks)
8. [Summary](#summary)

---

## WHY: The Linear Systems Revolution

### Linear Systems Are Everywhere

The problem $A\mathbf{x} = \mathbf{b}$ appears in:

| Domain | Application | Matrix Size |
|--------|-------------|-------------|
| **Machine Learning** | Regression, PCA | $10^6 \times 10^6$ |
| **Simulation** | Fluid dynamics, FEM | $10^9 \times 10^9$ |
| **Finance** | Portfolio optimization | $10^4 \times 10^4$ |
| **Network Analysis** | PageRank | $10^{10} \times 10^{10}$ |
| **Quantum Chemistry** | Electronic structure | $10^6 \times 10^6$ |

### The Classical Wall

Best classical algorithms for solving $n \times n$ systems:

| Method | Time Complexity | When Useful |
|--------|----------------|-------------|
| Gaussian Elimination | $O(n^3)$ | Dense matrices |
| Conjugate Gradient | $O(n\sqrt{\kappa})$ | Sparse, symmetric |
| Direct Methods | $O(n^{2.37})$ | Dense (Strassen-like) |

For $n = 10^9$ variables: **years of computation** on supercomputers.

### The HHL Promise

**Harrow-Hassidim-Lloyd (2009)** algorithm achieves:
$$O(\log(n) \cdot s^2 \cdot \kappa^2 / \epsilon)$$

Where:
- $\log(n)$: Exponentially better than $n$!
- $s$: Sparsity of $A$
- $\kappa$: Condition number of $A$
- $\epsilon$: Error tolerance

> **‚ö†Ô∏è Important**: This speedup comes with MAJOR caveats. The "fine print" matters enormously.

### WHEN to Use HHL

| ‚úÖ HHL May Help | ‚ùå HHL Won't Help |
|-----------------|-------------------|
| Quantum input data | Classical input data (qRAM problem) |
| Only need expectation values | Need full solution vector |
| Well-conditioned matrices ($\kappa$ small) | Ill-conditioned matrices |
| Sparse matrices | Dense matrices |
| Part of larger quantum algorithm | Standalone classical problem |

---

## WHAT: HHL Core Concepts

### üî∑ Definition (7-Element Framework)

**HHL** (Harrow-Hassidim-Lloyd) is a quantum algorithm that:
1. Encodes $\mathbf{b}$ as quantum state $|b\rangle$
2. Uses **QPE** to extract eigenvalues $\lambda_j$ of $A$
3. Performs **controlled rotation** to invert eigenvalues: $\lambda_j \to 1/\lambda_j$
4. Uses **inverse QPE** to uncompute
5. Outputs $|x\rangle \propto A^{-1}|b\rangle$

### üé≠ Analogy: The Eigenvalue Inverter

Imagine you need to divide by many numbers simultaneously:

```
CLASSICAL APPROACH:
    Given: numbers [2, 5, 3, 8, ...]
    Compute: [1/2, 1/5, 1/3, 1/8, ...] one by one
    Time: O(n) divisions

HHL APPROACH:
    Encode all numbers in quantum eigenvalue register
    Single quantum operation inverts ALL simultaneously
    Like having infinite parallel division machines!
```

**The catch**: Getting numbers in and results out is the bottleneck.

### üìê Mathematical Foundation

#### The Setup

Given:
- Hermitian matrix $A$ (size $n \times n$)
- Vector $\mathbf{b}$ (length $n$)
- Goal: Find $\mathbf{x}$ such that $A\mathbf{x} = \mathbf{b}$

Solution: $\mathbf{x} = A^{-1}\mathbf{b}$

#### Spectral Decomposition

Since $A$ is Hermitian, it has eigendecomposition:
$$A = \sum_{j=0}^{n-1} \lambda_j |u_j\rangle\langle u_j|$$

where $|u_j\rangle$ are orthonormal eigenvectors with eigenvalues $\lambda_j$.

#### Inverting via Eigenvalues

The inverse is:
$$A^{-1} = \sum_{j=0}^{n-1} \frac{1}{\lambda_j} |u_j\rangle\langle u_j|$$

Expand $|b\rangle$ in eigenbasis:
$$|b\rangle = \sum_j \beta_j |u_j\rangle$$

Then:
$$|x\rangle = A^{-1}|b\rangle = \sum_j \frac{\beta_j}{\lambda_j} |u_j\rangle$$

**HHL's insight**: Use QPE to find $\lambda_j$, then rotate to encode $1/\lambda_j$.

### üñ•Ô∏è HHL Circuit Structure

```
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
|0‚ü©^‚äót ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   QPE   ‚îú‚îÄ‚îÄ‚î§ Controlled Rot  ‚îú‚îÄ‚îÄ‚î§ QPE^{-1}  ‚îú‚îÄ‚îÄ (uncompute)
              ‚îÇ         ‚îÇ  ‚îÇ    (invert Œª)   ‚îÇ  ‚îÇ           ‚îÇ
              ‚îÇ         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ           ‚îÇ
|b‚ü© ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§           ‚îú‚îÄ‚îÄ |x‚ü©
              ‚îÇ         ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
|0‚ü© ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Ancilla (flag)
                                  Rotate
                                 by 1/Œª

```

---

## The HHL Algorithm Step-by-Step

### Step 1: State Preparation

Encode the known vector $\mathbf{b}$ as a quantum state:
$$|b\rangle = \sum_{j=0}^{n-1} \beta_j |u_j\rangle$$

**Challenge**: This requires $O(\log n)$ qubits but loading classical data may need $O(n)$ operations (the "input problem").

### Step 2: Quantum Phase Estimation (QPE)

Apply QPE with unitary $U = e^{iA}$ to extract eigenvalues:

$$|0\rangle^{\otimes t}|b\rangle \xrightarrow{QPE} \sum_j \beta_j |\tilde{\lambda}_j\rangle |u_j\rangle$$

Where $|\tilde{\lambda}_j\rangle$ is a $t$-qubit binary approximation of $\lambda_j$.

**Precision**: $t$ qubits give precision $2^{-t}$ in the eigenvalue.

### Step 3: Controlled Rotation (The Magic Step)

Apply a controlled rotation on an ancilla qubit:

$$|\tilde{\lambda}_j\rangle|0\rangle \to |\tilde{\lambda}_j\rangle\left(\sqrt{1-\frac{C^2}{\lambda_j^2}}|0\rangle + \frac{C}{\lambda_j}|1\rangle\right)$$

Where $C$ is a normalization constant (typically $C = \lambda_{min}$).

**Key insight**: The amplitude $C/\lambda_j$ encodes the inverse eigenvalue!

After this step:
$$\sum_j \beta_j |\tilde{\lambda}_j\rangle |u_j\rangle \left(\sqrt{1-\frac{C^2}{\lambda_j^2}}|0\rangle + \frac{C}{\lambda_j}|1\rangle\right)$$

### Step 4: Inverse QPE (Uncompute)

Apply $QPE^{-1}$ to disentangle the eigenvalue register:

$$\to \sum_j \beta_j |0\rangle^{\otimes t} |u_j\rangle \left(\sqrt{1-\frac{C^2}{\lambda_j^2}}|0\rangle + \frac{C}{\lambda_j}|1\rangle\right)$$

### Step 5: Measurement (Post-Selection)

Measure the ancilla qubit:
- If ancilla = $|1\rangle$: Success! Data register contains $|x\rangle \propto A^{-1}|b\rangle$
- If ancilla = $|0\rangle$: Failure. Repeat.

**Success probability**: $\propto ||A^{-1}|b\rangle||^2$

When successful:
$$|x\rangle = \frac{1}{\sqrt{\sum_j |\beta_j/\lambda_j|^2}} \sum_j \frac{\beta_j}{\lambda_j}|u_j\rangle = A^{-1}|b\rangle / ||A^{-1}|b\rangle||$$

---

## Critical Caveats

### Caveat 1: The Input Problem (qRAM)

**Problem**: Loading classical vector $\mathbf{b}$ into $|b\rangle$ requires $O(n)$ operations.

```
Classical data b = [b‚ÇÄ, b‚ÇÅ, ..., b_{n-1}]
                      ‚Üì
              ‚ùì How to load efficiently?
                      ‚Üì
Quantum state |b‚ü© = Œ£ b‚±º|j‚ü©
```

**Solutions**:
- **qRAM**: Hypothetical quantum memory (not yet built)
- **Quantum data**: If $|b\rangle$ comes from another quantum process
- **Specific structures**: Some $\mathbf{b}$ can be prepared efficiently

> **Reality check**: If you have to classically load $\mathbf{b}$, you lose the quantum speedup!

### Caveat 2: The Output Problem

**Problem**: You get $|x\rangle$, not $\mathbf{x}$!

Reading out all $n$ components of $\mathbf{x}$ requires $O(n)$ measurements.

**When HHL still helps**:
- You only need $\langle x | M | x \rangle$ for some observable $M$
- $|x\rangle$ feeds into another quantum algorithm
- You need to sample from the solution distribution

```
HHL outputs: |x‚ü© = quantum state

What you CAN efficiently get:
  ‚Ä¢ Expectation values ‚ü®x|M|x‚ü©
  ‚Ä¢ Samples from |x|¬≤ distribution
  ‚Ä¢ Use as input to other quantum algorithms

What you CANNOT efficiently get:
  ‚Ä¢ All n components of x
  ‚Ä¢ The full classical solution vector
```

### Caveat 3: Condition Number Dependence

The complexity depends on $\kappa = \lambda_{max}/\lambda_{min}$.

| Condition Number | HHL Complexity | Classical (CG) | Winner |
|-----------------|----------------|----------------|--------|
| $\kappa = 10$ | $O(\log n \cdot 100)$ | $O(n \cdot 3)$ | HHL (if $n$ large) |
| $\kappa = 10^6$ | $O(\log n \cdot 10^{12})$ | $O(n \cdot 1000)$ | Classical! |

**Rule of thumb**: HHL needs $\kappa = O(\text{poly}(\log n))$ for advantage.

### Caveat 4: Sparsity Requirement

HHL requires $A$ to be $s$-sparse (at most $s$ nonzero entries per row).

Complexity: $O(\log n \cdot s^2 \cdot \kappa^2)$

For dense matrices ($s = n$), the speedup disappears.

### Summary Table: When Does HHL Win?

| Requirement | Satisfied? | Impact |
|------------|------------|--------|
| Quantum input (no qRAM) | ‚úÖ Yes | Full speedup |
| Only need expectation values | ‚úÖ Yes | Full speedup |
| Low condition number ($\kappa$) | ‚úÖ $\kappa = O(\text{poly}\log n)$ | Full speedup |
| Sparse matrix | ‚úÖ $s = O(\text{poly}\log n)$ | Full speedup |
| **All satisfied** | ‚úÖ | **Exponential advantage** |
| **Any violated** | ‚ùå | Speedup reduced or eliminated |

---

## Implementation Details

### Simplified 2√ó2 HHL

For a 2√ó2 Hermitian matrix, we can implement HHL with:
- 1 data qubit (encodes $|b\rangle$)
- 2 clock qubits (eigenvalue register)
- 1 ancilla qubit (success flag)

```python
from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
import numpy as np

def hhl_2x2_circuit(A, b):
    """
    HHL for 2√ó2 Hermitian matrix.
    
    A must have eigenvalues in (0, 1] for this simplified version.
    """
    # Find eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(A)
    
    # Registers
    clock = QuantumRegister(2, 'clock')  # Eigenvalue register
    data = QuantumRegister(1, 'data')    # |b‚ü©
    ancilla = QuantumRegister(1, 'anc')  # Success flag
    c = ClassicalRegister(1, 'c')
    
    qc = QuantumCircuit(clock, data, ancilla, c)
    
    # Step 1: Prepare |b‚ü© (simplified - assume |b‚ü© = |0‚ü© or |1‚ü©)
    # For general |b‚ü©, use state preparation circuit
    
    # Step 2: QPE to extract eigenvalues
    # (Simplified version using known eigenvectors)
    qc.h(clock)
    # Controlled-U operations based on A's structure
    
    # Step 3: Controlled rotation
    # Angle depends on eigenvalue in clock register
    
    # Step 4: Inverse QPE
    qc.h(clock)
    
    # Step 5: Measure ancilla
    qc.measure(ancilla, c)
    
    return qc
```

### Controlled Rotation Implementation

The controlled rotation encodes $1/\lambda$ in the amplitude:

```python
def controlled_rotation(qc, clock_reg, ancilla):
    """
    Controlled Y-rotation based on eigenvalue register.
    
    For eigenvalue Œª encoded in clock, rotate ancilla by:
    Œ∏ = 2*arcsin(C/Œª)
    
    This puts amplitude C/Œª on |1‚ü© state.
    """
    # For 2-qubit clock with eigenvalues Œª‚ÇÅ, Œª‚ÇÇ:
    # |00‚ü© ‚Üí eigenvalue Œª‚ÇÅ
    # |01‚ü© ‚Üí eigenvalue Œª‚ÇÇ
    
    C = 0.5  # Normalization constant (‚â§ min eigenvalue)
    
    # Different rotation for each eigenvalue
    for state, eigenvalue in [(0b00, lambda1), (0b01, lambda2)]:
        angle = 2 * np.arcsin(C / eigenvalue)
        # Multi-controlled Ry rotation
        # ... (implementation depends on Qiskit version)
```

---

## Trap Alerts üö®

### Trap 1: Ignoring the Input Problem
```python
# ‚ùå WRONG THINKING: "HHL gives O(log n) speedup!"
# You forgot that loading classical data takes O(n)

# ‚úÖ CORRECT: Account for data loading
# If |b‚ü© comes from quantum source: O(log n) ‚úì
# If |b‚ü© loaded from classical: O(n) loading + O(log n) HHL = O(n) total
```

### Trap 2: Expecting Full Solution Vector
```python
# ‚ùå WRONG: "HHL will give me all n components of x"
# Measurement collapse prevents this!

# ‚úÖ CORRECT: Only expect
# - Expectation values ‚ü®x|M|x‚ü©
# - Samples from solution distribution
# - Feed into subsequent quantum operations
```

### Trap 3: High Condition Number
```python
# ‚ùå WRONG: "My matrix has Œ∫ = 10^8, HHL will still be fast"
# Complexity is O(Œ∫¬≤), so this is 10^16 factor!

# ‚úÖ CORRECT: Pre-condition your matrix if possible
# Or check if classical methods are actually faster
```

### Trap 4: Non-Hermitian Matrices
```python
# ‚ùå WRONG: Directly applying HHL to non-Hermitian A

# ‚úÖ CORRECT: Convert to Hermitian form
# A_eff = [[0, A], [A‚Ä†, 0]]
# Solve A_eff @ [0, x]^T = [b, 0]^T
```

### Trap 5: Forgetting Normalization
```python
# ‚ùå WRONG: Expecting |x‚ü© = A^{-1}|b‚ü© exactly

# ‚úÖ CORRECT: HHL outputs NORMALIZED state
# |x‚ü© = A^{-1}|b‚ü© / ||A^{-1}|b‚ü©||
# You get the direction, not the magnitude!
```

---

## Mnemonic: QPERM üîê

Remember HHL with **QPERM**:

| Letter | Step | Details |
|--------|------|---------|
| **Q** | **QPE** to extract eigenvalues | $\|b\rangle \to \sum_j \beta_j \|\lambda_j\rangle\|u_j\rangle$ |
| **P** | **Prepare** controlled rotation | Set up ancilla for inversion |
| **E** | **Execute** rotation (invert $\lambda$) | Ancilla amplitude $\propto 1/\lambda$ |
| **R** | **Reverse** QPE (uncompute) | Disentangle eigenvalue register |
| **M** | **Measure** ancilla for success | $\|1\rangle$ means success, $\|0\rangle$ retry |

---

## Quick Checks ‚úÖ

### üü¢ Basic Understanding

1. **What is the main speedup of HHL over classical methods?**
   <details>
   <summary>Answer</summary>
   HHL achieves $O(\log n)$ complexity in the matrix dimension, compared to classical $O(n)$ or $O(n^3)$. This is exponential speedup in terms of $n$.
   </details>

2. **Why is QPE essential for HHL?**
   <details>
   <summary>Answer</summary>
   QPE extracts the eigenvalues of matrix $A$ into a quantum register. These eigenvalues are needed to perform the inversion: we rotate an ancilla by an angle proportional to $1/\lambda$ based on the eigenvalue register.
   </details>

### üü° Intermediate Application

3. **If the smallest eigenvalue of A is 0.01 and largest is 1.0, what is the condition number?**
   <details>
   <summary>Answer</summary>
   $\kappa = \lambda_{max}/\lambda_{min} = 1.0/0.01 = 100$
   
   HHL complexity scales as $O(\kappa^2) = O(10,000)$, which is manageable.
   </details>

4. **You run HHL and measure the ancilla as |0‚ü©. What happened?**
   <details>
   <summary>Answer</summary>
   This is a "failure" outcome. The controlled rotation puts amplitude $\sqrt{1 - C^2/\lambda^2}$ on $|0\rangle$ and $C/\lambda$ on $|1\rangle$. Measuring $|0\rangle$ means we didn't get the inverted solution. We must restart and try again (post-selection).
   </details>

### üî¥ Advanced Analysis

5. **Why doesn't HHL provide advantage for dense matrices?**
   <details>
   <summary>Answer</summary>
   HHL complexity is $O(\log n \cdot s^2 \cdot \kappa^2)$ where $s$ is sparsity. For dense matrices, $s = n$, so complexity becomes $O(n^2 \log n \cdot \kappa^2)$, which is worse than classical $O(n^3)$ only when $\kappa^2 < n / \log n$.
   </details>

6. **A company claims their quantum computer will solve 10^9-dimensional systems using HHL in seconds. What questions should you ask?**
   <details>
   <summary>Answer</summary>
   1. How is the input vector loaded? (qRAM doesn't exist)
   2. What output do they extract? (Full vector takes O(n) measurements)
   3. What is the condition number? (High Œ∫ eliminates speedup)
   4. Is the matrix sparse? (Dense matrices lose the speedup)
   5. How many qubits and what error rates? (NISQ limitations)
   </details>

---

## Historical Context

### The HHL Paper (2009)

Aram Harrow, Avinatan Hassidim, and Seth Lloyd published "Quantum algorithm for linear systems of equations" showing:

- First exponential speedup for a "useful" linear algebra problem
- Sparked intense research into quantum machine learning
- Led to the "dequantization" field (Tang 2018 found classical algorithms matching some quantum ML)

### The Dequantization Era (2018+)

Ewin Tang showed that if you have "sample and query" access to data (similar to what HHL needs), classical algorithms can sometimes match quantum speedups. This led to re-evaluation of quantum ML advantage claims.

**Lesson**: Always carefully analyze the assumptions behind speedup claims!

---

## Beyond Basic HHL

### Improvements and Variants

| Variant | Improvement | Reference |
|---------|-------------|-----------|
| **Variable-time HHL** | Better $\kappa$ dependence | Ambainis (2012) |
| **QLSA** | Optimal $\kappa$ scaling | Childs et al. (2017) |
| **Quantum-inspired classical** | Dequantization | Tang (2018) |
| **Block-encoding HHL** | Better gate complexity | Chakraborty et al. (2018) |

### Applications Where HHL Shines

1. **Quantum simulations**: Solving Schr√∂dinger equation (data is already quantum)
2. **Quantum PCA**: Finding principal components of quantum states
3. **Quantum ML subroutines**: Within larger quantum algorithms where output feeds into more quantum processing

---

## Summary

### Key Takeaways

1. **HHL solves $A\mathbf{x} = \mathbf{b}$ in $O(\log n)$** ... with major caveats

2. **The circuit: QPE ‚Üí Rotate ‚Üí Inverse QPE ‚Üí Measure**
   - QPE extracts eigenvalues
   - Rotation inverts them: $\lambda \to 1/\lambda$
   - Measurement post-selects successful inversion

3. **Critical caveats to remember**:
   - **Input**: Loading classical data costs $O(n)$
   - **Output**: Only get quantum state, not classical vector
   - **Condition number**: Complexity is $O(\kappa^2)$
   - **Sparsity**: Need $s = O(\text{poly}\log n)$

4. **When HHL wins**: Quantum input, quantum output, low $\kappa$, sparse $A$

5. **Honest assessment**: True exponential advantage rare in practice, but HHL remains foundational for quantum linear algebra

### What's Next?

- **Codelab**: Implement simplified HHL in [11_hhl_codelab.ipynb](codelabs/11_hhl_codelab.ipynb)
- **Next**: [Quantum Neural Networks](Module-12-Quantum-ML.md)
- **Extensions**: QLSA improvements, applications in quantum chemistry

---

## References

1. Harrow, A. W., Hassidim, A., & Lloyd, S. (2009). "Quantum algorithm for linear systems of equations." Physical Review Letters, 103(15), 150502.
2. Childs, A. M., Kothari, R., & Somma, R. D. (2017). "Quantum algorithm for systems of linear equations with exponentially improved dependence on precision."
3. Tang, E. (2019). "A quantum-inspired classical algorithm for recommendation systems."
4. Aaronson, S. (2015). "Read the fine print." Nature Physics, 11(4), 291-293.

---

*Part of the Interactive Quantum Computing Learning Series*
