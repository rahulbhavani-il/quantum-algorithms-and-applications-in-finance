{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af22a73",
   "metadata": {},
   "source": [
    "# ðŸ§  QNN Codelab: Quantum Neural Networks\n",
    "\n",
    "## Building and Training Variational Quantum Classifiers\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Algorithm** | Quantum Neural Network (Variational Classifier) |\n",
    "| **Difficulty** | ðŸ”´ Advanced |\n",
    "| **Time** | 90-120 minutes |\n",
    "| **Prerequisites** | Module-12-Quantum-ML.md, VQE concepts |\n",
    "| **Qiskit Version** | 2.x |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By completing this codelab, you will:\n",
    "\n",
    "1. âœ… Implement feature maps for data encoding\n",
    "2. âœ… Build variational ansÃ¤tze with trainable parameters\n",
    "3. âœ… Compute gradients using the parameter shift rule\n",
    "4. âœ… Train a QNN classifier on toy datasets\n",
    "5. âœ… Compare QNN performance to classical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fb1f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import pi\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister, transpile\n",
    "from qiskit.primitives import StatevectorSampler\n",
    "from qiskit_ibm_runtime.fake_provider import FakeAlmadenV2\n",
    "from qiskit.quantum_info import Statevector, Operator\n",
    "from qiskit.visualization import plot_histogram, plot_bloch_multivector\n",
    "\n",
    "# Install scikit-learn\n",
    "%pip install scikit-learn\n",
    "\n",
    "# ML tools\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Initialize fake backend and sampler\n",
    "fake_backend = FakeAlmadenV2()\n",
    "sampler = StatevectorSampler()\n",
    "\n",
    "# Version check\n",
    "import qiskit\n",
    "print(f\"Qiskit version: {qiskit.__version__}\")\n",
    "assert int(qiskit.__version__.split('.')[0]) >= 1, \"Requires Qiskit 1.x or 2.x\"\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "print(\"âœ… All imports successful!\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b3fa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Theory Recap\n",
    "\n",
    "### QNN Architecture\n",
    "\n",
    "$$f_\\theta(x) = \\langle 0 | U^\\dagger(x) V^\\dagger(\\theta) Z V(\\theta) U(x) | 0 \\rangle$$\n",
    "\n",
    "Where:\n",
    "- $U(x)$: Feature map (encodes input data)\n",
    "- $V(\\theta)$: Variational ansatz (trainable)\n",
    "- $Z$: Measurement observable\n",
    "\n",
    "### Parameter Shift Rule\n",
    "\n",
    "For gradient computation:\n",
    "$$\\frac{\\partial f}{\\partial \\theta} = \\frac{f(\\theta + \\pi/2) - f(\\theta - \\pi/2)}{2}$$\n",
    "\n",
    "### Training\n",
    "\n",
    "Minimize loss: $\\mathcal{L} = \\sum_i (y_i - f_\\theta(x_i))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0174f53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Create Toy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed218d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    \"\"\"\n",
    "    Create toy classification datasets.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Datasets with train/test splits\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Moons dataset (non-linear)\n",
    "    X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "    \n",
    "    # Scale to [0, Ï€] for angle encoding\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "    \n",
    "    # Convert labels to {-1, +1}\n",
    "    y_moons_binary = 2 * y_moons - 1\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_moons_scaled, y_moons_binary, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    datasets['moons'] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'X_full': X_moons_scaled,\n",
    "        'y_full': y_moons_binary\n",
    "    }\n",
    "    \n",
    "    # Linearly separable dataset (for sanity check)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    X_linear = np.random.randn(n_samples, 2)\n",
    "    y_linear = np.sign(X_linear[:, 0] + X_linear[:, 1])  # Line x + y = 0\n",
    "    y_linear[y_linear == 0] = 1  # Handle edge case\n",
    "    \n",
    "    X_linear_scaled = scaler.fit_transform(X_linear)\n",
    "    \n",
    "    X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "        X_linear_scaled, y_linear, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    datasets['linear'] = {\n",
    "        'X_train': X_train_lin,\n",
    "        'X_test': X_test_lin,\n",
    "        'y_train': y_train_lin,\n",
    "        'y_test': y_test_lin\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Create datasets\n",
    "datasets = create_datasets()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, (name, data) in zip(axes, datasets.items()):\n",
    "    if 'X_full' in data:\n",
    "        X, y = data['X_full'], data['y_full']\n",
    "    else:\n",
    "        X = np.vstack([data['X_train'], data['X_test']])\n",
    "        y = np.hstack([data['y_train'], data['y_test']])\n",
    "    \n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', marker='o', label='Class +1')\n",
    "    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='red', marker='x', label='Class -1')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(f'{name.capitalize()} Dataset')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"  {name}: {len(data['X_train'])} train, {len(data['X_test'])} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b183d5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Feature Map Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66357274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_encoding(qc: QuantumCircuit, x: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Simple angle encoding: each feature becomes a rotation angle.\n",
    "    \n",
    "    Parameters:\n",
    "        qc: QuantumCircuit to modify\n",
    "        x: Input features (array of length n_qubits)\n",
    "    \"\"\"\n",
    "    n_qubits = len(x)\n",
    "    for i in range(n_qubits):\n",
    "        qc.ry(x[i], i)\n",
    "\n",
    "def zz_feature_map(qc: QuantumCircuit, x: np.ndarray, reps: int = 2) -> None:\n",
    "    \"\"\"\n",
    "    ZZ Feature Map with entanglement.\n",
    "    \n",
    "    Encodes data with single-qubit rotations and ZZ entanglement.\n",
    "    \n",
    "    Parameters:\n",
    "        qc: QuantumCircuit to modify\n",
    "        x: Input features\n",
    "        reps: Number of repetitions\n",
    "    \"\"\"\n",
    "    n_qubits = len(x)\n",
    "    \n",
    "    for _ in range(reps):\n",
    "        # Hadamard layer\n",
    "        for i in range(n_qubits):\n",
    "            qc.h(i)\n",
    "        \n",
    "        # Single-qubit rotation layer\n",
    "        for i in range(n_qubits):\n",
    "            qc.rz(2 * x[i], i)\n",
    "        \n",
    "        # ZZ entanglement layer\n",
    "        for i in range(n_qubits - 1):\n",
    "            qc.cx(i, i + 1)\n",
    "            qc.rz(2 * x[i] * x[i + 1], i + 1)\n",
    "            qc.cx(i, i + 1)\n",
    "\n",
    "# Demonstrate feature maps\n",
    "x_sample = np.array([np.pi/4, np.pi/3])\n",
    "\n",
    "# Angle encoding\n",
    "qc_angle = QuantumCircuit(2)\n",
    "angle_encoding(qc_angle, x_sample)\n",
    "\n",
    "print(\"Angle Encoding:\")\n",
    "print(qc_angle.draw(output='text'))\n",
    "\n",
    "# ZZ feature map\n",
    "qc_zz = QuantumCircuit(2)\n",
    "zz_feature_map(qc_zz, x_sample, reps=1)\n",
    "\n",
    "print(\"\\nZZ Feature Map (1 rep):\")\n",
    "print(qc_zz.draw(output='text'))\n",
    "\n",
    "# Compare resulting states\n",
    "print(\"\\nðŸ“Š Resulting States:\")\n",
    "print(f\"  Angle encoding: {Statevector(qc_angle).data.round(3)}\")\n",
    "print(f\"  ZZ feature map: {Statevector(qc_zz).data.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98233fd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Variational Ansatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_layer(qc: QuantumCircuit, params: np.ndarray, n_qubits: int) -> None:\n",
    "    \"\"\"\n",
    "    Single variational layer with rotations and entanglement.\n",
    "    \n",
    "    Parameters:\n",
    "        qc: QuantumCircuit to modify\n",
    "        params: Array of 3*n_qubits parameters [Rx, Ry, Rz for each qubit]\n",
    "        n_qubits: Number of qubits\n",
    "    \"\"\"\n",
    "    # Rotation layer (Rx, Ry, Rz per qubit)\n",
    "    for i in range(n_qubits):\n",
    "        qc.rx(params[3*i], i)\n",
    "        qc.ry(params[3*i + 1], i)\n",
    "        qc.rz(params[3*i + 2], i)\n",
    "    \n",
    "    # Entanglement layer (CNOT chain)\n",
    "    for i in range(n_qubits - 1):\n",
    "        qc.cx(i, i + 1)\n",
    "    # Wrap around for cyclic entanglement\n",
    "    if n_qubits > 2:\n",
    "        qc.cx(n_qubits - 1, 0)\n",
    "\n",
    "def variational_ansatz(qc: QuantumCircuit, params: np.ndarray, n_qubits: int, n_layers: int) -> None:\n",
    "    \"\"\"\n",
    "    Multi-layer variational ansatz.\n",
    "    \n",
    "    Parameters:\n",
    "        qc: QuantumCircuit to modify\n",
    "        params: All parameters (3 * n_qubits * n_layers total)\n",
    "        n_qubits: Number of qubits\n",
    "        n_layers: Number of variational layers\n",
    "    \"\"\"\n",
    "    params_per_layer = 3 * n_qubits\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        layer_params = params[layer * params_per_layer : (layer + 1) * params_per_layer]\n",
    "        variational_layer(qc, layer_params, n_qubits)\n",
    "\n",
    "# Demonstrate ansatz\n",
    "n_qubits = 2\n",
    "n_layers = 2\n",
    "n_params = 3 * n_qubits * n_layers\n",
    "\n",
    "test_params = np.random.uniform(0, 2*np.pi, n_params)\n",
    "\n",
    "qc_ansatz = QuantumCircuit(n_qubits)\n",
    "variational_ansatz(qc_ansatz, test_params, n_qubits, n_layers)\n",
    "\n",
    "print(f\"Variational Ansatz ({n_layers} layers, {n_params} parameters):\")\n",
    "print(qc_ansatz.draw(output='text'))\n",
    "\n",
    "print(f\"\\nðŸ“Š Ansatz Statistics:\")\n",
    "print(f\"  Qubits: {n_qubits}\")\n",
    "print(f\"  Layers: {n_layers}\")\n",
    "print(f\"  Parameters: {n_params}\")\n",
    "print(f\"  Circuit depth: {qc_ansatz.depth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36301f22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5.5: State Evolution Analysis\n",
    "\n",
    "### 5.5.1 The \"Feature Map + Variational Layer\" Rule\n",
    "\n",
    "**The Key Identity**: For a quantum neural network classifier:\n",
    "\n",
    "$$f_\\theta(x) = \\langle 0 | U^\\dagger(x) V^\\dagger(\\theta) M V(\\theta) U(x) | 0 \\rangle$$\n",
    "\n",
    "Where:\n",
    "- $U(x)$: **Feature map** - encodes classical data $x$ into quantum state\n",
    "- $V(\\theta)$: **Variational ansatz** - trainable quantum circuit with parameters $\\theta$\n",
    "- $M$: **Measurement operator** - typically Pauli Z on first qubit\n",
    "\n",
    "**The Rule**: \"Encode data into quantum amplitudes/phases, transform with trainable rotations, extract prediction via measurement.\"\n",
    "\n",
    "**From the Lecture**:\n",
    "> \"Think of this like VQE type of circuit, where I will perform unitary rotations on a bunch of qubits. And the amount of rotation is given by some parameters Î¸. These Î¸ could serve the purpose of weight matrix in classical neural networks.\"\n",
    "\n",
    "**Why This Works**:\n",
    "\n",
    "1. **Feature map creates data-dependent state**: $|x\\rangle = U(x)|0\\rangle$\n",
    "2. **Variational layer transforms**: $|\\psi(\\theta)\\rangle = V(\\theta)|x\\rangle$  \n",
    "3. **Measurement extracts prediction**: $f = \\langle\\psi|M|\\psi\\rangle \\in [-1, 1]$\n",
    "4. **Non-linearity from measurement**: $P(0) = |\\langle 0|\\psi\\rangle|^2$ is non-linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cce89d",
   "metadata": {},
   "source": [
    "### 5.5.2 State Evolution Through a QNN\n",
    "\n",
    "| Stage | State | Physical Meaning |\n",
    "|-------|-------|------------------|\n",
    "| **Initial** | $\\|0\\rangle^{\\otimes n}$ | Blank quantum register |\n",
    "| **After Feature Map** | $\\|x\\rangle = U(x)\\|0\\rangle$ | Data encoded as quantum state |\n",
    "| **After Ansatz** | $\\|\\psi(\\theta)\\rangle = V(\\theta)\\|x\\rangle$ | Trainable transformation applied |\n",
    "| **Measurement** | $\\langle Z\\rangle = \\langle\\psi\\|Z\\|\\psi\\rangle$ | Prediction extracted as expectation value |\n",
    "\n",
    "**Key Insight from Lecture**:\n",
    "> \"All these unitary transformations are linear transformations. But I do need some non-linearity like that neuron. That nonlinearity comes eventually by final measurement. Measurement collapses the stateâ€”it is not a linear transformation.\"\n",
    "\n",
    "**Why Measurement Provides Non-Linearity**:\n",
    "- Unitary operations: $|\\psi\\rangle \\to U|\\psi\\rangle$ (linear in amplitudes)\n",
    "- Measurement probability: $P(0) = |\\alpha|^2$ (non-linear in amplitudes)\n",
    "- This is analogous to activation functions in classical NNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_qnn_evolution(\n",
    "    x: np.ndarray, \n",
    "    params: np.ndarray, \n",
    "    n_qubits: int = 2, \n",
    "    n_layers: int = 1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trace state evolution through a QNN step by step.\n",
    "    \n",
    "    Demonstrates the \"Feature Map + Variational Layer\" rule.\n",
    "    \n",
    "    Parameters:\n",
    "        x: Input features\n",
    "        params: Variational parameters\n",
    "        n_qubits: Number of qubits\n",
    "        n_layers: Number of variational layers\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QNN STATE EVOLUTION TRACE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nðŸ“¥ INPUT:\")\n",
    "    print(f\"   Classical data x = {x}\")\n",
    "    print(f\"   Variational parameters Î¸ = {params[:6]}...\" if len(params) > 6 else f\"   Î¸ = {params}\")\n",
    "    \n",
    "    # Stage 1: Initial state\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    state_0 = Statevector(qc)\n",
    "    print(f\"\\nðŸ”µ STAGE 1: Initial State |0âŸ©^âŠ—{n_qubits}\")\n",
    "    print(f\"   |Ïˆâ‚€âŸ© = {state_0.data}\")\n",
    "    \n",
    "    # Stage 2: After feature map\n",
    "    angle_encoding(qc, x)\n",
    "    state_x = Statevector(qc)\n",
    "    print(f\"\\nðŸŸ¢ STAGE 2: After Feature Map U(x)\")\n",
    "    print(f\"   Applied: R_Y(x_i) on each qubit\")\n",
    "    print(f\"   |xâŸ© = U(x)|0âŸ©\")\n",
    "    print(f\"   Amplitudes: {np.round(state_x.data, 4)}\")\n",
    "    \n",
    "    # Analyze feature encoding\n",
    "    print(f\"\\n   ðŸ“Š Feature Encoding Analysis:\")\n",
    "    for i, xi in enumerate(x):\n",
    "        print(f\"      Qubit {i}: R_Y({xi:.4f}) rotates |0âŸ© toward |1âŸ©\")\n",
    "        print(f\"               Creates cos({xi/2:.3f})|0âŸ© + sin({xi/2:.3f})|1âŸ©\")\n",
    "    \n",
    "    # Stage 3: After variational ansatz\n",
    "    qc_full = QuantumCircuit(n_qubits)\n",
    "    angle_encoding(qc_full, x)\n",
    "    variational_ansatz(qc_full, params, n_qubits, n_layers)\n",
    "    state_theta = Statevector(qc_full)\n",
    "    \n",
    "    print(f\"\\nðŸŸ¡ STAGE 3: After Variational Ansatz V(Î¸)\")\n",
    "    print(f\"   |Ïˆ(Î¸)âŸ© = V(Î¸)|xâŸ©\")\n",
    "    print(f\"   Amplitudes: {np.round(state_theta.data, 4)}\")\n",
    "    \n",
    "    # Compute expectation value\n",
    "    Z = np.array([[1, 0], [0, -1]])\n",
    "    identity = np.eye(2)\n",
    "    \n",
    "    # Z on first qubit, identity on rest\n",
    "    Z_full = Z\n",
    "    for _ in range(n_qubits - 1):\n",
    "        Z_full = np.kron(identity, Z_full)\n",
    "    \n",
    "    exp_z = np.real(state_theta.data.conj() @ Z_full @ state_theta.data)\n",
    "    \n",
    "    print(f\"\\nðŸ”´ STAGE 4: Measurement âŸ¨Zâ‚€âŸ©\")\n",
    "    print(f\"   Observable: Z on qubit 0\")\n",
    "    print(f\"   âŸ¨ZâŸ© = âŸ¨Ïˆ(Î¸)|Z|Ïˆ(Î¸)âŸ© = {exp_z:.4f}\")\n",
    "    print(f\"   Prediction: {'Class +1' if exp_z > 0 else 'Class -1'}\")\n",
    "    \n",
    "    # Show Bloch sphere for 2-qubit case\n",
    "    if n_qubits == 2:\n",
    "        print(f\"\\nðŸ“ˆ PROBABILITY DISTRIBUTION:\")\n",
    "        probs = state_theta.probabilities()\n",
    "        for i, p in enumerate(probs):\n",
    "            basis = format(i, f'0{n_qubits}b')\n",
    "            bar = 'â–ˆ' * int(p * 40)\n",
    "            print(f\"   |{basis}âŸ©: {p:.4f} {bar}\")\n",
    "    \n",
    "    return exp_z\n",
    "\n",
    "# Demonstrate with sample data\n",
    "print(\"Tracing QNN evolution for a single input point...\\n\")\n",
    "\n",
    "x_demo = np.array([np.pi/4, np.pi/3])  # Sample input\n",
    "params_demo = np.random.uniform(0, np.pi, 3 * 2 * 1)  # 2 qubits, 1 layer\n",
    "\n",
    "prediction = trace_qnn_evolution(x_demo, params_demo, n_qubits=2, n_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5073823",
   "metadata": {},
   "source": [
    "### 5.5.3 Understanding Feature Maps: How Data Becomes Quantum\n",
    "\n",
    "Different encoding strategies create different quantum \"feature spaces\":\n",
    "\n",
    "| Encoding | Transformation | Use Case |\n",
    "|----------|----------------|----------|\n",
    "| **Angle** | $R_Y(x_i)\\|0\\rangle$ | Simple, efficient, 1 qubit per feature |\n",
    "| **Amplitude** | $\\sum_i x_i\\|i\\rangle$ | Efficient for high-dim data, needs normalization |\n",
    "| **ZZ** | $H \\cdot R_Z(x) \\cdot ZZ(x_i x_j)$ | Captures feature interactions |\n",
    "| **Basis** | $\\|x\\rangle$ for binary $x$ | Discrete/binary data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab724f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_feature_maps(x: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Compare how different feature maps encode the same data point.\n",
    "    \n",
    "    From the lecture:\n",
    "    \"At the input I give some input state whose features I want to learn. \n",
    "    This could be a classical vector loaded inside the quantum computer as a quantum state.\"\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE MAP COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nðŸ“¥ Input data: x = {x}\")\n",
    "    print(f\"   (Scaled to [0, Ï€] for encoding)\")\n",
    "    \n",
    "    n_qubits = len(x)\n",
    "    \n",
    "    # 1. Angle encoding\n",
    "    qc_angle = QuantumCircuit(n_qubits)\n",
    "    angle_encoding(qc_angle, x)\n",
    "    state_angle = Statevector(qc_angle)\n",
    "    \n",
    "    print(f\"\\nðŸ”µ ANGLE ENCODING: R_Y(x_i)|0âŸ©\")\n",
    "    print(f\"   Circuit depth: {qc_angle.depth()}\")\n",
    "    print(f\"   State: {np.round(state_angle.data, 4)}\")\n",
    "    \n",
    "    # 2. ZZ feature map\n",
    "    qc_zz = QuantumCircuit(n_qubits)\n",
    "    zz_feature_map(qc_zz, x, reps=1)\n",
    "    state_zz = Statevector(qc_zz)\n",
    "    \n",
    "    print(f\"\\nðŸŸ¢ ZZ FEATURE MAP: H â†’ R_Z(x) â†’ ZZ(x_iÂ·x_j)\")\n",
    "    print(f\"   Circuit depth: {qc_zz.depth()}\")\n",
    "    print(f\"   State: {np.round(state_zz.data, 4)}\")\n",
    "    \n",
    "    # 3. Compute inner products (quantum kernels)\n",
    "    # K(x, x') = |âŸ¨Ï†(x)|Ï†(x')âŸ©|Â²\n",
    "    \n",
    "    # Compare with shifted data point\n",
    "    x_shifted = x + 0.3\n",
    "    \n",
    "    # Angle kernel\n",
    "    qc_angle_shifted = QuantumCircuit(n_qubits)\n",
    "    angle_encoding(qc_angle_shifted, x_shifted)\n",
    "    state_angle_shifted = Statevector(qc_angle_shifted)\n",
    "    kernel_angle = np.abs(np.vdot(state_angle.data, state_angle_shifted.data))**2\n",
    "    \n",
    "    # ZZ kernel\n",
    "    qc_zz_shifted = QuantumCircuit(n_qubits)\n",
    "    zz_feature_map(qc_zz_shifted, x_shifted, reps=1)\n",
    "    state_zz_shifted = Statevector(qc_zz_shifted)\n",
    "    kernel_zz = np.abs(np.vdot(state_zz.data, state_zz_shifted.data))**2\n",
    "    \n",
    "    print(f\"\\nðŸ“Š QUANTUM KERNEL: K(x, x') = |âŸ¨Ï†(x)|Ï†(x')âŸ©|Â²\")\n",
    "    print(f\"   (Measuring similarity between x and x + 0.3)\")\n",
    "    print(f\"   Angle encoding kernel: K = {kernel_angle:.4f}\")\n",
    "    print(f\"   ZZ feature map kernel: K = {kernel_zz:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Probabilities for angle encoding\n",
    "    ax1 = axes[0]\n",
    "    probs_angle = state_angle.probabilities()\n",
    "    bars = ax1.bar(range(len(probs_angle)), probs_angle, color='steelblue')\n",
    "    ax1.set_xlabel('Basis state')\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.set_title('Angle Encoding')\n",
    "    ax1.set_xticks(range(len(probs_angle)))\n",
    "    ax1.set_xticklabels([format(i, f'0{n_qubits}b') for i in range(len(probs_angle))])\n",
    "    \n",
    "    # Probabilities for ZZ encoding\n",
    "    ax2 = axes[1]\n",
    "    probs_zz = state_zz.probabilities()\n",
    "    bars = ax2.bar(range(len(probs_zz)), probs_zz, color='coral')\n",
    "    ax2.set_xlabel('Basis state')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.set_title('ZZ Feature Map')\n",
    "    ax2.set_xticks(range(len(probs_zz)))\n",
    "    ax2.set_xticklabels([format(i, f'0{n_qubits}b') for i in range(len(probs_zz))])\n",
    "    \n",
    "    # Kernel comparison\n",
    "    ax3 = axes[2]\n",
    "    x_range = np.linspace(0, np.pi, 20)\n",
    "    kernels_angle = []\n",
    "    kernels_zz = []\n",
    "    \n",
    "    for x_new in x_range:\n",
    "        x_test = np.array([x_new, x[1]])  # Vary first feature\n",
    "        \n",
    "        qc_test = QuantumCircuit(n_qubits)\n",
    "        angle_encoding(qc_test, x_test)\n",
    "        k = np.abs(np.vdot(state_angle.data, Statevector(qc_test).data))**2\n",
    "        kernels_angle.append(k)\n",
    "        \n",
    "        qc_test_zz = QuantumCircuit(n_qubits)\n",
    "        zz_feature_map(qc_test_zz, x_test, reps=1)\n",
    "        k_zz = np.abs(np.vdot(state_zz.data, Statevector(qc_test_zz).data))**2\n",
    "        kernels_zz.append(k_zz)\n",
    "    \n",
    "    ax3.plot(x_range, kernels_angle, 'b-', linewidth=2, label='Angle')\n",
    "    ax3.plot(x_range, kernels_zz, 'r--', linewidth=2, label='ZZ')\n",
    "    ax3.axvline(x=x[0], color='gray', linestyle=':', alpha=0.5, label=f'xâ‚={x[0]:.2f}')\n",
    "    ax3.set_xlabel('xâ‚ value')\n",
    "    ax3.set_ylabel('Kernel K(x, x\\')')\n",
    "    ax3.set_title('Kernel Similarity Decay')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ”‘ KEY INSIGHT:\")\n",
    "    print(\"   - ZZ feature map creates more complex kernel due to entanglement\")\n",
    "    print(\"   - Feature maps define the 'quantum feature space' for classification\")\n",
    "    print(\"   - Different maps work better for different data distributions\")\n",
    "\n",
    "compare_feature_maps(np.array([np.pi/4, np.pi/3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d341b",
   "metadata": {},
   "source": [
    "### 5.5.4 Interactive: Explore the Loss Landscape\n",
    "\n",
    "The **loss function** drives training:\n",
    "$$\\mathcal{L}(\\theta) = \\sum_i \\left( y_i - f_\\theta(x_i) \\right)^2$$\n",
    "\n",
    "From the lecture:\n",
    "> \"Given the parameters Î¸ and the input state, if this expectation value is exactly same as the label L, then the loss will be 0. But in reality we will measure some loss because this expectation value will be deviated away from this ideal value. Our job is to minimize this loss.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8345709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_qnn_loss_landscape():\n",
    "    \"\"\"\n",
    "    Visualize the loss landscape of a QNN for a simple classification task.\n",
    "    \n",
    "    Demonstrates barren plateaus and local minima.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"QNN LOSS LANDSCAPE EXPLORATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simple 2-point classification\n",
    "    X_simple = np.array([[0.5, 0.5], [2.5, 2.5]])  # Two points\n",
    "    y_simple = np.array([1, -1])  # Class +1 and -1\n",
    "    \n",
    "    print(f\"\\nðŸ“¥ Simple classification task:\")\n",
    "    print(f\"   Point 1: x={X_simple[0]}, y=+1\")\n",
    "    print(f\"   Point 2: x={X_simple[1]}, y=-1\")\n",
    "    \n",
    "    # For 2 qubits, 1 layer: 6 parameters\n",
    "    # Fix 4 parameters, vary 2 to visualize\n",
    "    n_qubits = 2\n",
    "    n_layers = 1\n",
    "    n_params = 3 * n_qubits * n_layers\n",
    "    \n",
    "    base_params = np.array([np.pi/4, np.pi/4, np.pi/4, np.pi/4, np.pi/4, np.pi/4])\n",
    "    \n",
    "    # Create meshgrid for first two parameters\n",
    "    theta_range = np.linspace(0, 2*np.pi, 25)\n",
    "    theta1, theta2 = np.meshgrid(theta_range, theta_range)\n",
    "    \n",
    "    print(f\"\\nâ³ Computing loss landscape (varying Î¸â‚€ and Î¸â‚)...\")\n",
    "    \n",
    "    loss_landscape = np.zeros_like(theta1)\n",
    "    \n",
    "    for i in range(theta1.shape[0]):\n",
    "        for j in range(theta1.shape[1]):\n",
    "            params = base_params.copy()\n",
    "            params[0] = theta1[i, j]\n",
    "            params[1] = theta2[i, j]\n",
    "            \n",
    "            # Compute loss for both points\n",
    "            total_loss = 0\n",
    "            for x, y in zip(X_simple, y_simple):\n",
    "                # Use statevector for fast evaluation\n",
    "                qc = QuantumCircuit(n_qubits)\n",
    "                angle_encoding(qc, x)\n",
    "                variational_ansatz(qc, params, n_qubits, n_layers)\n",
    "                state = Statevector(qc)\n",
    "                \n",
    "                # Compute âŸ¨ZâŸ© on first qubit\n",
    "                probs = state.probabilities()\n",
    "                # P(0 on first qubit) = sum of probs where first bit is 0\n",
    "                p0 = probs[0] + probs[2] if n_qubits == 2 else probs[0]\n",
    "                p1 = probs[1] + probs[3] if n_qubits == 2 else probs[1]\n",
    "                exp_z = p0 - p1\n",
    "                \n",
    "                total_loss += (y - exp_z)**2\n",
    "            \n",
    "            loss_landscape[i, j] = total_loss / 2\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 3D surface\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax1.plot_surface(theta1, theta2, loss_landscape, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_xlabel('Î¸â‚€')\n",
    "    ax1.set_ylabel('Î¸â‚')\n",
    "    ax1.set_zlabel('Loss')\n",
    "    ax1.set_title('QNN Loss Landscape (3D)')\n",
    "    \n",
    "    # Contour plot\n",
    "    ax2 = axes[1]\n",
    "    contour = ax2.contourf(theta1, theta2, loss_landscape, levels=20, cmap='viridis')\n",
    "    plt.colorbar(contour, ax=ax2, label='Loss')\n",
    "    ax2.set_xlabel('Î¸â‚€')\n",
    "    ax2.set_ylabel('Î¸â‚')\n",
    "    ax2.set_title('QNN Loss Landscape (Contour)')\n",
    "    \n",
    "    # Mark minimum\n",
    "    min_idx = np.unravel_index(np.argmin(loss_landscape), loss_landscape.shape)\n",
    "    ax2.plot(theta1[min_idx], theta2[min_idx], 'r*', markersize=15, label=f'Min Loss: {loss_landscape.min():.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Landscape Statistics:\")\n",
    "    print(f\"   Minimum loss: {loss_landscape.min():.4f} at Î¸â‚€={theta1[min_idx]:.2f}, Î¸â‚={theta2[min_idx]:.2f}\")\n",
    "    print(f\"   Maximum loss: {loss_landscape.max():.4f}\")\n",
    "    print(f\"   Mean loss: {loss_landscape.mean():.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ”‘ KEY INSIGHT (Barren Plateaus):\")\n",
    "    print(\"   For deeper circuits and more qubits, the landscape becomes\")\n",
    "    print(\"   increasingly flat â†’ gradients vanish â†’ training becomes hard!\")\n",
    "    print(\"   This is why shallow QNNs often work better than deep ones.\")\n",
    "\n",
    "explore_qnn_loss_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc2aa4",
   "metadata": {},
   "source": [
    "### 5.5.5 The Barren Plateau Problem: Why Deep QNNs Are Hard to Train\n",
    "\n",
    "From the lecture:\n",
    "> \"Whether there's an advantage over classical neural networksâ€”that prediction is still open. Most likely for classical type of tasks, the classical neural networks are already very well advanced.\"\n",
    "\n",
    "**The Problem**: For random parameterized circuits, gradient variance scales as:\n",
    "$$\\text{Var}\\left[\\frac{\\partial \\mathcal{L}}{\\partial \\theta}\\right] \\sim \\frac{1}{2^n}$$\n",
    "\n",
    "This means gradients become **exponentially small** with more qubits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d364f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_barren_plateaus():\n",
    "    \"\"\"\n",
    "    Demonstrate how gradient variance decreases with circuit complexity.\n",
    "    \n",
    "    The key insight from QML research: random circuits lead to barren plateaus\n",
    "    where gradients vanish exponentially with qubit count.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BARREN PLATEAU DEMONSTRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    def estimate_gradient_variance_exact(n_qubits, n_layers, n_samples=20):\n",
    "        \"\"\"Estimate gradient variance using statevector simulation.\"\"\"\n",
    "        n_params = 3 * n_qubits * n_layers\n",
    "        x_sample = np.random.uniform(0, np.pi, n_qubits)\n",
    "        \n",
    "        gradients = []\n",
    "        for _ in range(n_samples):\n",
    "            params = np.random.uniform(0, 2*np.pi, n_params)\n",
    "            \n",
    "            # Compute gradient of first parameter using parameter shift\n",
    "            params_plus = params.copy()\n",
    "            params_plus[0] += np.pi / 2\n",
    "            params_minus = params.copy()\n",
    "            params_minus[0] -= np.pi / 2\n",
    "            \n",
    "            # Forward pass +\n",
    "            qc_plus = QuantumCircuit(n_qubits)\n",
    "            angle_encoding(qc_plus, x_sample)\n",
    "            variational_ansatz(qc_plus, params_plus, n_qubits, n_layers)\n",
    "            state_plus = Statevector(qc_plus)\n",
    "            probs_plus = state_plus.probabilities()\n",
    "            exp_z_plus = sum((-1)**((i >> 0) & 1) * p for i, p in enumerate(probs_plus))\n",
    "            \n",
    "            # Forward pass -\n",
    "            qc_minus = QuantumCircuit(n_qubits)\n",
    "            angle_encoding(qc_minus, x_sample)\n",
    "            variational_ansatz(qc_minus, params_minus, n_qubits, n_layers)\n",
    "            state_minus = Statevector(qc_minus)\n",
    "            probs_minus = state_minus.probabilities()\n",
    "            exp_z_minus = sum((-1)**((i >> 0) & 1) * p for i, p in enumerate(probs_minus))\n",
    "            \n",
    "            grad = (exp_z_plus - exp_z_minus) / 2\n",
    "            gradients.append(grad)\n",
    "        \n",
    "        return np.var(gradients), np.mean(np.abs(gradients))\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        (2, 1, \"2 qubits, 1 layer\"),\n",
    "        (2, 2, \"2 qubits, 2 layers\"),\n",
    "        (2, 4, \"2 qubits, 4 layers\"),\n",
    "        (3, 2, \"3 qubits, 2 layers\"),\n",
    "        (4, 2, \"4 qubits, 2 layers\"),\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Configuration':<25} {'Var(âˆ‚f/âˆ‚Î¸)':<15} {'Mean |âˆ‡|':<15} {'Trainable?':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for n_q, n_l, desc in configs:\n",
    "        var, mean_abs = estimate_gradient_variance_exact(n_q, n_l, n_samples=15)\n",
    "        trainable = \"âœ“ Yes\" if var > 0.001 else \"âš  Difficult\" if var > 0.0001 else \"âœ— Very Hard\"\n",
    "        print(f\"{desc:<25} {var:<15.6f} {mean_abs:<15.6f} {trainable:<12}\")\n",
    "        results.append((desc, var, mean_abs))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Gradient variance vs config\n",
    "    ax1 = axes[0]\n",
    "    labels = [r[0] for r in results]\n",
    "    variances = [r[1] for r in results]\n",
    "    ax1.bar(range(len(labels)), variances, color='steelblue')\n",
    "    ax1.set_xticks(range(len(labels)))\n",
    "    ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Gradient Variance')\n",
    "    ax1.set_title('Gradient Variance vs Circuit Complexity')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.axhline(y=0.001, color='orange', linestyle='--', label='Trainability threshold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Theoretical scaling\n",
    "    ax2 = axes[1]\n",
    "    n_qubits_range = np.arange(2, 10)\n",
    "    theoretical_var = 1 / (2 ** n_qubits_range)  # Theoretical: Var ~ 1/2^n\n",
    "    \n",
    "    ax2.semilogy(n_qubits_range, theoretical_var, 'r-o', linewidth=2, markersize=8, label='Theoretical: 1/2â¿')\n",
    "    ax2.set_xlabel('Number of Qubits')\n",
    "    ax2.set_ylabel('Gradient Variance')\n",
    "    ax2.set_title('Barren Plateau: Exponential Vanishing')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Annotate\n",
    "    for n, v in zip(n_qubits_range, theoretical_var):\n",
    "        if n in [2, 5, 8]:\n",
    "            ax2.annotate(f'{v:.1e}', (n, v), textcoords=\"offset points\", xytext=(5, 5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ”‘ KEY INSIGHTS:\")\n",
    "    print(\"   1. Deeper circuits â†’ smaller gradient variance â†’ harder training\")\n",
    "    print(\"   2. More qubits â†’ exponentially smaller gradients (barren plateaus)\")\n",
    "    print(\"   3. Mitigation strategies:\")\n",
    "    print(\"      - Use shallow circuits (few layers)\")\n",
    "    print(\"      - Problem-specific ansÃ¤tze (not random)\")\n",
    "    print(\"      - Layer-wise training\")\n",
    "    print(\"      - Parameter initialization near identity\")\n",
    "\n",
    "demonstrate_barren_plateaus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a139440",
   "metadata": {},
   "source": [
    "### 5.5.6 Summary: How QNNs Work (and When They Might Help)\n",
    "\n",
    "| Component | Formula | Role in QNN |\n",
    "|-----------|---------|-------------|\n",
    "| **Feature Map** | $U(x)\\|0\\rangle$ | Encodes classical data into quantum state |\n",
    "| **Variational Layer** | $V(\\theta)$ | Trainable transformation (like weights in classical NN) |\n",
    "| **Measurement** | $\\langle Z \\rangle$ | Extracts prediction; provides non-linearity |\n",
    "| **Loss Function** | $\\mathcal{L} = \\sum_i (y_i - f_\\theta(x_i))^2$ | Drives optimization via gradient descent |\n",
    "| **Gradient** | Parameter shift: $\\frac{f(\\theta+\\pi/2) - f(\\theta-\\pi/2)}{2}$ | Exact gradient without finite differences |\n",
    "\n",
    "**The \"Feature Map + Variational Layer\" Rule**:\n",
    "> \"Encode data with U(x), apply trainable rotations V(Î¸), measure to get prediction. Optimize Î¸ to minimize loss. The key is that quantum feature spaces may separate data that classical methods cannot.\"\n",
    "\n",
    "**Key Insights from the Lecture**:\n",
    "\n",
    "1. **Why measurement is crucial**: \"Nonlinearity comes eventually by final measurement. Measurement collapses the stateâ€”it is not a linear transformation.\"\n",
    "\n",
    "2. **Quantum data advantage**: \"When your input is quantum in natureâ€”produced by some quantum computer or molecule or atomâ€”such quantum circuits which are able to learn could provide a very useful tool.\"\n",
    "\n",
    "3. **Hybrid approach**: \"You can now start to combine quantum and classical systems together and see if the combined neural network that you get is good at doing something which just the classical neural network was not able to do.\"\n",
    "\n",
    "**When QNNs Might Help**:\n",
    "- âœ… **Quantum data**: Input comes from quantum sensors/simulations\n",
    "- âœ… **Kernel advantage**: Quantum kernels separate data classical kernels cannot\n",
    "- âœ… **Small data regimes**: Where overfitting to large models is a concern\n",
    "- âœ… **As subroutines**: Within larger quantum algorithms\n",
    "\n",
    "**Challenges**:\n",
    "- âŒ **Barren plateaus**: Gradients vanish for deep/wide circuits\n",
    "- âŒ **Classical baselines**: SVMs and NNs are already very powerful\n",
    "- âŒ **Noise**: NISQ devices add errors to gradient estimates\n",
    "- âŒ **No proven advantage**: For classical data, quantum speedup not established"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acf5ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Complete QNN Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qnn_circuit(\n",
    "    x: np.ndarray,\n",
    "    params: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    feature_map: str = 'angle'\n",
    ") -> QuantumCircuit:\n",
    "    \"\"\"\n",
    "    Create complete QNN circuit.\n",
    "    \n",
    "    Parameters:\n",
    "        x: Input features\n",
    "        params: Variational parameters\n",
    "        n_qubits: Number of qubits\n",
    "        n_layers: Number of variational layers\n",
    "        feature_map: 'angle' or 'zz'\n",
    "    \n",
    "    Returns:\n",
    "        QuantumCircuit with QNN\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    \n",
    "    # Feature map\n",
    "    if feature_map == 'angle':\n",
    "        angle_encoding(qc, x)\n",
    "    elif feature_map == 'zz':\n",
    "        zz_feature_map(qc, x, reps=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature map: {feature_map}\")\n",
    "    \n",
    "    qc.barrier()\n",
    "    \n",
    "    # Variational ansatz\n",
    "    variational_ansatz(qc, params, n_qubits, n_layers)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "def qnn_expectation(\n",
    "    x: np.ndarray,\n",
    "    params: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    shots: int = 1024,\n",
    "    feature_map: str = 'angle'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute QNN expectation value âŸ¨ZâŸ© on first qubit.\n",
    "    \n",
    "    Parameters:\n",
    "        x: Input features\n",
    "        params: Variational parameters\n",
    "        n_qubits: Number of qubits\n",
    "        n_layers: Number of variational layers\n",
    "        shots: Number of measurement shots\n",
    "        feature_map: 'angle' or 'zz'\n",
    "    \n",
    "    Returns:\n",
    "        float: âŸ¨ZâŸ© expectation value in [-1, 1]\n",
    "    \"\"\"\n",
    "    qc = create_qnn_circuit(x, params, n_qubits, n_layers, feature_map)\n",
    "    qc.measure_all()\n",
    "    \n",
    "    # Transpile and run with SamplerV2\n",
    "    transpiled_qc = transpile(qc, backend=fake_backend, optimization_level=3)\n",
    "    job = sampler.run([transpiled_qc], shots=shots)\n",
    "    result = job.result()\n",
    "    counts = result[0].data.meas.get_counts()\n",
    "    \n",
    "    # Compute âŸ¨ZâŸ© on first qubit (rightmost in Qiskit)\n",
    "    exp_z = 0\n",
    "    for bitstring, count in counts.items():\n",
    "        # First qubit is rightmost\n",
    "        bit = int(bitstring[-1])\n",
    "        z_val = 1 - 2 * bit  # |0âŸ© â†’ +1, |1âŸ© â†’ -1\n",
    "        exp_z += z_val * count\n",
    "    \n",
    "    return exp_z / shots\n",
    "\n",
    "# Test QNN\n",
    "x_test = np.array([np.pi/4, np.pi/3])\n",
    "params_test = np.random.uniform(0, 2*np.pi, 3 * 2 * 2)  # 2 qubits, 2 layers\n",
    "\n",
    "qc_full = create_qnn_circuit(x_test, params_test, n_qubits=2, n_layers=2, feature_map='angle')\n",
    "print(\"Complete QNN Circuit:\")\n",
    "print(qc_full.draw(output='text'))\n",
    "\n",
    "exp_val = qnn_expectation(x_test, params_test, n_qubits=2, n_layers=2, shots=1024)\n",
    "\n",
    "print(f\"\\nâŸ¨ZâŸ© = {exp_val:.4f}\")\n",
    "print(f\"Prediction: {'Class +1' if exp_val > 0 else 'Class -1'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e4c21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Gradient Computation (Parameter Shift Rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e368fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(\n",
    "    x: np.ndarray,\n",
    "    params: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    shots: int = 1024,\n",
    "    feature_map: str = 'angle'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute gradient using parameter shift rule.\n",
    "    \n",
    "    âˆ‚f/âˆ‚Î¸ = (f(Î¸ + Ï€/2) - f(Î¸ - Ï€/2)) / 2\n",
    "    \n",
    "    Parameters:\n",
    "        x: Input features\n",
    "        params: Current parameters\n",
    "        n_qubits, n_layers, shots, feature_map: QNN config\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Gradient vector\n",
    "    \"\"\"\n",
    "    n_params = len(params)\n",
    "    gradient = np.zeros(n_params)\n",
    "    \n",
    "    for i in range(n_params):\n",
    "        # f(Î¸ + Ï€/2)\n",
    "        params_plus = params.copy()\n",
    "        params_plus[i] += np.pi / 2\n",
    "        f_plus = qnn_expectation(x, params_plus, n_qubits, n_layers, shots, feature_map)\n",
    "        \n",
    "        # f(Î¸ - Ï€/2)\n",
    "        params_minus = params.copy()\n",
    "        params_minus[i] -= np.pi / 2\n",
    "        f_minus = qnn_expectation(x, params_minus, n_qubits, n_layers, shots, feature_map)\n",
    "        \n",
    "        gradient[i] = (f_plus - f_minus) / 2\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Demonstrate gradient computation\n",
    "print(\"Computing gradient via parameter shift rule...\")\n",
    "grad = compute_gradient(x_test, params_test, n_qubits=2, n_layers=2, shots=2048)\n",
    "\n",
    "print(f\"\\nGradient vector ({len(grad)} parameters):\")\n",
    "for i, g in enumerate(grad):\n",
    "    print(f\"  âˆ‚f/âˆ‚Î¸_{i} = {g:+.4f}\")\n",
    "\n",
    "print(f\"\\nGradient magnitude: ||âˆ‡f|| = {np.linalg.norm(grad):.4f}\")\n",
    "print(f\"Circuit evaluations: {2 * len(params_test)} (2 per parameter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c70be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: QNN Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qnn(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    learning_rate: float = 0.1,\n",
    "    epochs: int = 50,\n",
    "    shots: int = 512,\n",
    "    feature_map: str = 'angle',\n",
    "    verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train QNN using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: Training features (n_samples, n_features)\n",
    "        y_train: Training labels in {-1, +1}\n",
    "        n_qubits, n_layers: QNN architecture\n",
    "        learning_rate: Gradient descent step size\n",
    "        epochs: Number of training epochs\n",
    "        shots: Shots per circuit evaluation\n",
    "        feature_map: 'angle' or 'zz'\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        dict: Trained parameters and history\n",
    "    \"\"\"\n",
    "    n_params = 3 * n_qubits * n_layers\n",
    "    params = np.random.uniform(0, np.pi, n_params)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Accumulate gradients over mini-batch\n",
    "        gradients = np.zeros(n_params)\n",
    "        \n",
    "        for x, y in zip(X_train, y_train):\n",
    "            # Forward pass\n",
    "            pred = qnn_expectation(x, params, n_qubits, n_layers, shots, feature_map)\n",
    "            \n",
    "            # Loss (MSE)\n",
    "            loss = (y - pred) ** 2\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Accuracy\n",
    "            pred_class = 1 if pred > 0 else -1\n",
    "            if pred_class == y:\n",
    "                correct += 1\n",
    "            \n",
    "            # Gradient (chain rule: âˆ‚L/âˆ‚Î¸ = âˆ‚L/âˆ‚f * âˆ‚f/âˆ‚Î¸)\n",
    "            dL_df = -2 * (y - pred)\n",
    "            grad = compute_gradient(x, params, n_qubits, n_layers, shots, feature_map)\n",
    "            gradients += dL_df * grad\n",
    "        \n",
    "        # Update parameters\n",
    "        params -= learning_rate * gradients / len(X_train)\n",
    "        \n",
    "        # Record history\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        accuracy = correct / len(X_train)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        if verbose and epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy*100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'params': params,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "def evaluate_qnn(\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    params: np.ndarray,\n",
    "    n_qubits: int,\n",
    "    n_layers: int,\n",
    "    shots: int = 1024,\n",
    "    feature_map: str = 'angle'\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate trained QNN on test set.\n",
    "    \n",
    "    Returns:\n",
    "        float: Test accuracy\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for x, y in zip(X_test, y_test):\n",
    "        pred = qnn_expectation(x, params, n_qubits, n_layers, shots, feature_map)\n",
    "        pred_class = 1 if pred > 0 else -1\n",
    "        if pred_class == y:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(X_test)\n",
    "\n",
    "print(\"Note: Full training can take several minutes due to circuit evaluations.\")\n",
    "print(\"We'll use a small subset for demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d5528",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Train QNN on Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use small subset for faster training\n",
    "n_train_samples = 20\n",
    "n_test_samples = 10\n",
    "\n",
    "data = datasets['moons']\n",
    "X_train_small = data['X_train'][:n_train_samples]\n",
    "y_train_small = data['y_train'][:n_train_samples]\n",
    "X_test_small = data['X_test'][:n_test_samples]\n",
    "y_test_small = data['y_test'][:n_test_samples]\n",
    "\n",
    "print(f\"Training QNN on {n_train_samples} samples...\")\n",
    "print(f\"Configuration: 2 qubits, 2 layers, angle encoding\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train QNN\n",
    "result = train_qnn(\n",
    "    X_train_small, y_train_small,\n",
    "    n_qubits=2, n_layers=2,\n",
    "    learning_rate=0.3,\n",
    "    epochs=30,\n",
    "    shots=256,\n",
    "    feature_map='angle',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_acc = evaluate_qnn(X_train_small, y_train_small, result['params'], 2, 2, 512, 'angle')\n",
    "test_acc = evaluate_qnn(X_test_small, y_test_small, result['params'], 2, 2, 512, 'angle')\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Results:\")\n",
    "print(f\"  Train accuracy: {train_acc*100:.1f}%\")\n",
    "print(f\"  Test accuracy: {test_acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7fb07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(result['history']['loss'], 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.set_title('QNN Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(np.array(result['history']['accuracy']) * 100, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('QNN Training Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=50, color='r', linestyle='--', label='Random guess')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44609830",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Classical Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with classical SVM\n",
    "print(\"Classical Baseline Comparison\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Linear SVM\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_linear.fit(X_train_small, y_train_small)\n",
    "linear_train_acc = svm_linear.score(X_train_small, y_train_small)\n",
    "linear_test_acc = svm_linear.score(X_test_small, y_test_small)\n",
    "\n",
    "# RBF SVM\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_rbf.fit(X_train_small, y_train_small)\n",
    "rbf_train_acc = svm_rbf.score(X_train_small, y_train_small)\n",
    "rbf_test_acc = svm_rbf.score(X_test_small, y_test_small)\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'Train Acc':<15} {'Test Acc':<15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'QNN (2q, 2L)':<20} {train_acc*100:<15.1f}% {test_acc*100:<15.1f}%\")\n",
    "print(f\"{'SVM (Linear)':<20} {linear_train_acc*100:<15.1f}% {linear_test_acc*100:<15.1f}%\")\n",
    "print(f\"{'SVM (RBF)':<20} {rbf_train_acc*100:<15.1f}% {rbf_test_acc*100:<15.1f}%\")\n",
    "\n",
    "print(\"\\nâš ï¸ Note: With more training data and epochs, QNN may improve.\")\n",
    "print(\"   Classical methods are strong baselines for structured data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2338f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Trap Demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš¨ TRAP 1: Barren Plateaus\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compute gradient variance for different circuit sizes\n",
    "def estimate_gradient_variance(n_qubits, n_layers, n_samples=10):\n",
    "    \"\"\"Estimate variance of gradients for random parameters.\"\"\"\n",
    "    n_params = 3 * n_qubits * n_layers\n",
    "    x_sample = np.random.uniform(0, np.pi, n_qubits)\n",
    "    \n",
    "    gradients = []\n",
    "    for _ in range(n_samples):\n",
    "        params = np.random.uniform(0, 2*np.pi, n_params)\n",
    "        grad = compute_gradient(x_sample, params, n_qubits, n_layers, shots=256)\n",
    "        gradients.append(grad[0])  # First parameter's gradient\n",
    "    \n",
    "    return np.var(gradients)\n",
    "\n",
    "print(\"\\nGradient variance vs circuit size:\")\n",
    "print(f\"{'n_qubits':<12} {'n_layers':<12} {'Var(âˆ‚f/âˆ‚Î¸)':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Small circuit\n",
    "var_small = estimate_gradient_variance(2, 1, n_samples=5)\n",
    "print(f\"{'2':<12} {'1':<12} {var_small:<15.6f}\")\n",
    "\n",
    "# Medium circuit\n",
    "var_medium = estimate_gradient_variance(2, 3, n_samples=5)\n",
    "print(f\"{'2':<12} {'3':<12} {var_medium:<15.6f}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Lesson: Deeper circuits â†’ smaller gradient variance â†’ harder training\")\n",
    "print(\"   For many qubits, gradients become exponentially small!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f17e10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: Exercises\n",
    "\n",
    "### ðŸŸ¢ Exercise 1: Try ZZ Feature Map\n",
    "\n",
    "Modify the training to use the ZZ feature map instead of angle encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train QNN with ZZ feature map\n",
    "# result_zz = train_qnn(\n",
    "#     X_train_small, y_train_small,\n",
    "#     n_qubits=2, n_layers=2,\n",
    "#     feature_map='zz',  # Change this!\n",
    "#     epochs=30\n",
    "# )\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a00a59",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Exercise 2: Learning Rate Sweep\n",
    "\n",
    "Try different learning rates and plot the final accuracy vs learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep learning rates [0.01, 0.1, 0.3, 0.5, 1.0]\n",
    "# Plot accuracy vs learning rate\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcf83a9",
   "metadata": {},
   "source": [
    "### ðŸ”´ Exercise 3: Decision Boundary Visualization\n",
    "\n",
    "Create a grid of points and classify each to visualize the QNN's decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create meshgrid and classify each point\n",
    "# xx, yy = np.meshgrid(np.linspace(0, np.pi, 20), np.linspace(0, np.pi, 20))\n",
    "# predictions = [[qnn_expectation([x, y], result['params'], ...) for x, y in zip(row_x, row_y)] \n",
    "#                for row_x, row_y in zip(xx, yy)]\n",
    "# plt.contourf(xx, yy, predictions)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189344c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 14: Quick Checks âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§  Quick Check Questions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        'q': \"1. How many circuit evaluations are needed to compute gradients for 12 parameters?\",\n",
    "        'a': \"24 evaluations (2 per parameter for parameter shift rule)\"\n",
    "    },\n",
    "    {\n",
    "        'q': \"2. What happens to gradient magnitude as you add more qubits?\",\n",
    "        'a': \"Gradients become exponentially small (barren plateaus) - variance scales as ~1/2^n for random circuits\"\n",
    "    },\n",
    "    {\n",
    "        'q': \"3. Why is measurement the 'non-linearity' in QNNs?\",\n",
    "        'a': \"Unitaries are linear, but measurement probabilities involve |amplitude|Â² - a non-linear operation. This lets QNNs approximate non-linear functions.\"\n",
    "    },\n",
    "    {\n",
    "        'q': \"4. When might a QNN outperform classical ML?\",\n",
    "        'a': \"When: (1) data is quantum in origin, (2) kernel methods help but classical kernels fail, (3) small data with complex patterns, (4) as part of larger quantum algorithm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for q_dict in questions:\n",
    "    print(f\"\\n{q_dict['q']}\")\n",
    "    input(\"Press Enter to see answer...\")\n",
    "    print(f\"Answer: {q_dict['a']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462e435",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 15: Summary and Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **QNN = Feature Map + Ansatz + Measurement**\n",
    "   - Encode data, apply trainable rotations, measure expectation\n",
    "\n",
    "2. **Parameter Shift Rule gives exact gradients**\n",
    "   - Evaluate at Î¸ Â± Ï€/2, take difference\n",
    "   - Cost: 2 circuit evaluations per parameter\n",
    "\n",
    "3. **Barren plateaus limit scalability**\n",
    "   - Deep + many qubits = vanishing gradients\n",
    "   - Stay shallow, use problem-specific ansÃ¤tze\n",
    "\n",
    "4. **Always compare to classical baselines**\n",
    "   - SVM, random forest often competitive\n",
    "   - Quantum advantage not guaranteed\n",
    "\n",
    "### What You've Implemented\n",
    "\n",
    "- âœ… Feature maps (angle, ZZ)\n",
    "- âœ… Variational ansatz with rotations + entanglement\n",
    "- âœ… Gradient computation via parameter shift\n",
    "- âœ… Complete training loop with SGD\n",
    "- âœ… Comparison to classical methods\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Advanced**: Quantum kernel methods, QSVM\n",
    "- **Research**: Provable quantum advantages\n",
    "- **Applications**: Quantum chemistry, optimization\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've completed the Quantum Algorithms module series!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
